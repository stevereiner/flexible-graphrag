# Flexible GraphRAG Configuration

# Data Source (filesystem, cmis, alfresco, upload, web, wikipedia, youtube, s3, gcs, azure_blob, onedrive, sharepoint, box, google_drive)
DATA_SOURCE=filesystem
SOURCE_PATHS=["./sample-docs/cmispress.txt"]

# Example configurations for different data sources:
# WEB_CONFIG={"url": "https://example.com/page"}
# WIKIPEDIA_CONFIG={"query": "artificial intelligence", "language": "en", "max_docs": 1}
# YOUTUBE_CONFIG={"url": "https://www.youtube.com/watch?v=VIDEO_ID", "chunk_size_seconds": 60}
# S3_CONFIG={"bucket": "my-bucket", "key": "documents/", "region_name": "us-east-1"}
# GCS_CONFIG={"bucket": "my-bucket", "key": "documents/", "project_id": "my-project"}
# AZURE_BLOB_CONFIG={"container_name": "documents", "account_name": "myaccount"}
# ONEDRIVE_CONFIG={"client_id": "your-client-id", "client_secret": "your-secret", "tenant_id": "your-tenant"}
# SHAREPOINT_CONFIG={"client_id": "your-client-id", "client_secret": "your-secret", "tenant_id": "your-tenant", "sharepoint_site_name": "your-site"}
# BOX_CONFIG={"access_token": "your-access-token", "box_folder_id": "0"}
# GOOGLE_DRIVE_CONFIG={"folder_id": "your-folder-id", "credentials_path": "path/to/credentials.json"}

# Sample Text for Testing (optional - overrides default Dune text)
# SAMPLE_TEXT="Luke Skywalker is a Jedi Knight from Tatooine. His father is Darth Vader, formerly known as Anakin Skywalker."

# Source Path Examples:
# Windows single file: SOURCE_PATHS=["C:\Documents\report.pdf"]
# Windows multiple files: SOURCE_PATHS=["C:\file1.pdf", "D:\folder\file2.docx"] 
# Windows whole directory: SOURCE_PATHS=["C:\Documents\reports"]  # Note: processes ALL files
# macOS single file: SOURCE_PATHS=["/Users/username/Documents/report.pdf"]
# Linux single file: SOURCE_PATHS=["/home/username/documents/report.pdf"]
# See docs/SOURCE-PATH-EXAMPLES.md for more detailed examples
# Note: UI clients use different env vars (PROCESS_FOLDER_PATH, VITE_PROCESS_FOLDER_PATH)

# ====================================================================
# 1. GRAPH DATABASE CONFIGURATION
# ====================================================================

GRAPH_DB=neo4j
#GRAPH_DB=kuzu
#GRAPH_DB=falkordb
#GRAPH_DB=arcadedb
#GRAPH_DB=memgraph
#GRAPH_DB=nebula
#GRAPH_DB=neptune
#GRAPH_DB=neptune_analytics
#GRAPH_DB=none

# Enable knowledge graph extraction 
#ENABLE_KNOWLEDGE_GRAPH=false
ENABLE_KNOWLEDGE_GRAPH=true

# Knowledge graph extractor type: "simple", "schema", or "dynamic"
# simple: Basic extraction without schema constraints (uses SimpleLLMPathExtractor)
# schema: Uses predefined schema for structured extraction (uses SchemaLLMPathExtractor) - default
# Use SchemaLLMPathExtractor with strict=True when you have a well-defined domain and want to ensure consistency
# Use SchemaLLMPathExtractor with strict=False when you want some flexibility while still being guided by a base schema
# dynamic: Flexible extraction that can expand beyond initial schema (uses DynamicLLMPathExtractor)
# (DynamicLLMPathExtractor currently openai only, doesn't work on ollama)
# Use DynamicLLMPathExtractor when you want a balance between structure and flexibility, 
# especially for discovering new entity and relation types
KG_EXTRACTOR_TYPE=schema

# Graph Database Connection Configurations:

# Neo4j
GRAPH_DB_CONFIG={"url": "bolt://localhost:7687", "username": "neo4j", "password": "password"}

# Kuzu (database file will be created as ./kuzu_db/database.kz)
# use_structured_schema: false (default) - enables initial entity types and schema (not generic entity type schema)
# use_vector_index: false (default) - enables Kuzu's built-in vector capabilities use internally
GRAPH_DB_CONFIG={"db_path": "./kuzu_db/database.kz", "use_structured_schema": false, "use_vector_index": false}

# FalkorDB
#GRAPH_DB_CONFIG={"url": "falkor://localhost:6379"}
# With authentication:
#GRAPH_DB_CONFIG={"url": "falkor://localhost:6379", "username": "user", "password": "pass"}

# ArcadeDB (multi-model database with graph capabilities)
# include_basic_schema: True (default) - initial types: PERSON, ORGANIZATION, LOCATION, PLACE + Entity, TextChunk, MENTIONS
#                       False - initial types: Entity, TextChunk, MENTIONS only
#                       For both settings: LlamaIndex PathExtractors/KG_EXTRACTOR_TYPE control additional types
#GRAPH_DB_CONFIG={"host": "localhost", "port": 2480, "username": "root", "password": "playwithdata", "database": "flexible_graphrag", "include_basic_schema": true}
#GRAPH_DB_CONFIG={"host": "localhost", "port": 2480, "username": "root", "password": "playwithdata", "database": "flexible_graphrag", "include_basic_schema": false}

# MemGraph (real-time graph database)
# Basic configuration:
#GRAPH_DB_CONFIG={"url": "bolt://localhost:7688", "username": "", "password": ""}
# Full configuration with database parameter:
#GRAPH_DB_CONFIG={"url": "bolt://localhost:7688", "username": "", "password": "", "database": "memgraph"}

# NebulaGraph (distributed graph database)
# Basic configuration (uses defaults for connection):
#GRAPH_DB_CONFIG={"space": "flexible_graphrag", "overwrite": true}
# Full configuration with connection parameters (address/port format):
#GRAPH_DB_CONFIG={"space": "flexible_graphrag", "overwrite": true, "address": "localhost", "port": 9669, "username": "root", "password": "nebula"}
# Full configuration with URL format:
#GRAPH_DB_CONFIG={"space": "flexible_graphrag", "overwrite": true, "url": "nebula://localhost:9669", "username": "root", "password": "nebula"}
# Alternative format (space_name also supported for backward compatibility):
#GRAPH_DB_CONFIG={"space_name": "flexible_graphrag", "address": "localhost", "port": 9669, "username": "root", "password": "nebula"}

# Amazon Neptune (managed graph database service)
# With explicit AWS credentials:
#GRAPH_DB_CONFIG={"host": "your-neptune-cluster.cluster-xyz.us-east-1.neptune.amazonaws.com", "port": 8182, "region": "us-east-1", "access_key": "your_access_key", "secret_key": "your_secret_key"}
# With AWS credentials profile (alternative approach):
#GRAPH_DB_CONFIG={"host": "your-neptune-cluster.cluster-xyz.us-east-1.neptune.amazonaws.com", "port": 8182, "region": "us-east-1", "credentials_profile_name": "my-aws-profile"}

# Amazon Neptune Analytics (serverless graph analytics engine)
# NOTE: Neptune Analytics has non-atomic vector index limitations
# The system automatically sets embed_kg_nodes=False for Neptune Analytics to avoid vector conflicts
# Use a separate VECTOR_DB for embeddings when using Neptune Analytics
# With explicit AWS credentials:
#GRAPH_DB_CONFIG={"graph_identifier": "g-1234567890", "region": "us-east-1", "access_key": "your_access_key", "secret_key": "your_secret_key"}
# With AWS credentials profile (alternative approach):
#GRAPH_DB_CONFIG={"graph_identifier": "g-1234567890", "region": "us-east-1", "credentials_profile_name": "my-aws-profile"}
# Using default AWS credentials (from environment variables, IAM role, etc.):
#GRAPH_DB_CONFIG={"graph_identifier": "g-1234567890", "region": "us-east-1"}
# Note: Neptune Analytics requires a region to be specified
# Recommended: Use Neptune Analytics for graph + separate VECTOR_DB for embeddings


# ====================================================================
# SCHEMA CONFIGURATION - Controls entity and relationship extraction
# ====================================================================

# Schema set to default uses SAMPLE_SCHEMA in config.py
# "entities": Literal["PERSON", "ORGANIZATION", "LOCATION", "TECHNOLOGY", "PROJECT", "DOCUMENT"],
# "relations": Literal["WORKS_FOR", "LOCATED_IN", "USES", "COLLABORATES_WITH", "DEVELOPS", "MENTIONS"],
SCHEMA_NAME=default
#SCHEMA_NAME=none
#SCHEMA_NAME=cmis_press

# Custom schema example for SchemaLLMPathExtractor (and DynamicLLMPathExtractor)
#SCHEMAS=[{"name": "cmis_press", "schema": {"entities": ["PERSON", "ORGANIZATION", "TECHNOLOGY", "SPECIFICATION", "CONCEPT"], "relations": ["WORKS_FOR", "DEVELOPS", "SUPPORTS", "IMPLEMENTS", "COLLABORATES_WITH", "ANNOUNCES"], "validation_schema": [["PERSON", "WORKS_FOR", "ORGANIZATION"], ["PERSON", "DEVELOPS", "TECHNOLOGY"], ["ORGANIZATION", "SUPPORTS", "SPECIFICATION"], ["ORGANIZATION", "IMPLEMENTS", "TECHNOLOGY"], ["ORGANIZATION", "COLLABORATES_WITH", "ORGANIZATION"], ["ORGANIZATION", "ANNOUNCES", "SPECIFICATION"]], "strict": false}}]

# ====================================================================
# 2. VECTOR DATABASE CONFIGURATION  
# ====================================================================

#VECTOR_DB=neo4j
VECTOR_DB=qdrant
#VECTOR_DB=elasticsearch
#VECTOR_DB=opensearch
#VECTOR_DB=chroma
#VECTOR_DB=milvus
#VECTOR_DB=weaviate
#VECTOR_DB=pinecone
#VECTOR_DB=postgres
#VECTOR_DB=lancedb
#VECTOR_DB=none

# Vector Database Connection Configurations:

# Qdrant
VECTOR_DB_CONFIG={"host": "localhost", "port": 6333, "collection_name": "hybrid_search_vector", "https": false}

# Elasticsearch
#VECTOR_DB_CONFIG={"url": "http://localhost:9200", "index_name": "hybrid_search_vector"}

# OpenSearch Configuration (Vector Store) - Dense vector search
#VECTOR_DB_CONFIG={"url": "http://localhost:9201", "index_name": "hybrid_search_vector"}

# Neo4j VECTOR database configuration (separate from graph)
#VECTOR_DB_CONFIG={"url": "bolt://localhost:7687", "username": "neo4j", "password": "password", "index_name": "hybrid_search_vector", "database": "neo4j"}

# Chroma (local vector database with persistence)
# Local mode (file-based storage):
#VECTOR_DB_CONFIG={"persist_directory": "./chroma_db", "collection_name": "hybrid_search"}
# HTTP mode (connect to remote ChromaDB server):
#VECTOR_DB_CONFIG={"host": "localhost", "port": 8001, "collection_name": "hybrid_search"}

# Milvus (scalable vector database)
#VECTOR_DB_CONFIG={"host": "localhost", "port": 19530, "collection_name": "hybrid_search", "username": "root", "password": "milvus"}

# Weaviate (vector search engine with semantic capabilities)
# For local Docker instance (no authentication):
#VECTOR_DB_CONFIG={"url": "http://localhost:8081", "index_name": "HybridSearch"}
# For authenticated instance (with API key):
#VECTOR_DB_CONFIG={"url": "http://localhost:8081", "index_name": "HybridSearch", "api_key": "your_weaviate_api_key"}

# Pinecone (managed serverless vector database service)
# Sign up at https://app.pinecone.io (free starter plan available)
# Note: dimension is auto-detected from your embedding model, don't include it in config
#VECTOR_DB_CONFIG={"api_key": "your_pinecone_api_key", "region": "us-east-1", "cloud": "aws", "index_name": "hybrid-search", "metric": "cosine"}

# PostgreSQL with pgvector extension
#VECTOR_DB_CONFIG={"host": "localhost", "port": 5433, "database": "postgres", "username": "postgres", "password": "password", "table_name": "hybrid_search_vectors"}

# LanceDB (modern embedded vector database)
#VECTOR_DB_CONFIG={"uri": "./lancedb", "table_name": "hybrid_search", "vector_column_name": "vector", "text_column_name": "text"}


# ====================================================================
# 3. SEARCH DATABASE CONFIGURATION
# ====================================================================

#SEARCH_DB=bm25
SEARCH_DB=elasticsearch
#SEARCH_DB=opensearch
#SEARCH_DB=none

# Search Database Connection Configurations:
# Elasticsearch
SEARCH_DB_CONFIG={"url": "http://localhost:9200", "index_name": "hybrid_search_fulltext"}

# OpenSearch Configuration (Search Store) - BM25 fulltext search
#SEARCH_DB_CONFIG={"url": "http://localhost:9201", "index_name": "hybrid_search_fulltext"}

# ====================================================================
# 4. LLM CONFIGURATION
# ====================================================================

LLM_PROVIDER=openai
USE_OPENAI=true
#LLM_PROVIDER=ollama
#USE_OPENAI=false

# OpenAI Configuration (if using OpenAI)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small  # 1536 dimensions, recommended
# OPENAI_TIMEOUT=120.0  # LLM request timeout in seconds (default: 2 minutes)

# Ollama Configuration (if using Ollama)
#OLLAMA_MODEL=llama3.1:8b              # 8B params - did better previously, but gives errors,
                                       # may be my ollama env setting (4.9 GB on disk)

#OLLAMA_MODEL=llama3.2:3b              # 3B params - good balance, recommended default 
                                       # gets orgs, misses person cmispress.txt (2.0 GB on disk)

#OLLAMA_MODEL=gpt-oss:20b              # 20B params, needs 16 GB min GPU memory, and 16 GB - 32 GB system memory, 
                                       # 13 GB on disk, didn't seem to work (chunk only)

# Embedding Models for Ollama (smaller models perform better):
#EMBEDDING_MODEL=all-minilm             # 22M params, 384 dims - fastest, default for Ollama
#EMBEDDING_MODEL=nomic-embed-text       # 137M params, 768 dims - good balance of speed/quality
#EMBEDDING_MODEL=mxbai-embed-large      # 334M params, 1024 dims - high quality but much slower

#OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_TIMEOUT=300.0  # LLM request timeout in seconds (default: 5 minutes - higher for local processing)

# Azure OpenAI Configuration (if using Azure)
# AZURE_OPENAI_TIMEOUT=120.0  # LLM request timeout in seconds

# Anthropic Configuration (if using Claude)
# ANTHROPIC_TIMEOUT=120.0  # LLM request timeout in seconds

# Gemini Configuration (if using Google)
# GEMINI_TIMEOUT=120.0  # LLM request timeout in seconds

# ====================================================================
# 5. CONTENT SOURCES CONFIGURATION
# ====================================================================

# CMIS Configuration (if using CMIS)
CMIS_URL=http://localhost:8080/alfresco/api/-default-/public/cmis/versions/1.1/atom
CMIS_USERNAME=admin
CMIS_PASSWORD=admin

# Alfresco Configuration (if using Alfresco)
ALFRESCO_URL=http://localhost:8080/alfresco
ALFRESCO_USERNAME=admin
ALFRESCO_PASSWORD=admin

# Amazon S3 Configuration (if using S3)
# Note: These values can be configured in UI or set here as defaults
# UI values take precedence over these environment variables
# S3_REGION_NAME=us-east-2           # AWS region for S3 bucket (default: us-east-2)
# S3_BUCKET_NAME=my-bucket           # Default bucket name (used if not specified in UI)
# S3_PREFIX=documents/               # Default prefix/path within bucket
# S3_PREFIX=""                       # Or leave empty for whole bucket (top level)
# S3_ACCESS_KEY=myaccesskeyid        # AWS access key ID (used if not specified in UI)
# S3_SECRET_KEY=mysecretaccesskey    # AWS secret access key (used if not specified in UI)

# ====================================================================
# DATABASE CONNECTION DETAILS (Individual Configs)
# ====================================================================

# Neo4j Configuration (for both vector and graph storage)
# Browser URL: http://localhost:7474/browser (for database management and queries)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
# NEO4J_DATABASE=neo4j  # Optional: specify database name (default: neo4j)

# Neo4j AuraDB (cloud) example:
# NEO4J_URI=neo4j+s://<dbid>.databases.neo4j.io
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=<aura-generated-password>
# Console URL: https://console.neo4j.io

# Elasticsearch Configuration
ELASTICSEARCH_URL=http://localhost:9200
ELASTICSEARCH_USERNAME=
ELASTICSEARCH_PASSWORD=

# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=
# QDRANT_COLLECTION=hybrid_search  # Optional: collection name
# QDRANT_HTTPS=false  # Use HTTPS for remote Qdrant instances

# Weaviate Configuration
WEAVIATE_URL=http://localhost:8081
WEAVIATE_INDEX_NAME=HybridSearch
# WEAVIATE_API_KEY=your_weaviate_api_key  # Optional: for authenticated instances
# WEAVIATE_TEXT_KEY=content  # Optional: field name for text content

# OpenSearch Configuration
OPENSEARCH_URL=http://localhost:9201
OPENSEARCH_USERNAME=
OPENSEARCH_PASSWORD=

# Processing Configuration
CHUNK_SIZE=1024
CHUNK_OVERLAP=128

# Knowledge Graph Extraction Limits (configurable for different content densities)
# MAX_TRIPLETS_PER_CHUNK: Used by DynamicLLMPathExtractor and SchemaLLMPathExtractor - controls how many entity-relationship triplets can be extracted per text chunk
# MAX_PATHS_PER_CHUNK: Used by SimpleLLMPathExtractor - controls how many relationship paths can be extracted per text chunk
# Higher values allow more comprehensive extraction from dense content but may increase processing time
# Lower values are faster but may miss entities/relationships in complex documents
# These values are higher than the usual 20-25 seen in examples, are more to rule out
# that this is what is limiting extraction.
MAX_TRIPLETS_PER_CHUNK=100
MAX_PATHS_PER_CHUNK=100

# Timeout configurations moved to docs/TIMEOUT-CONFIGURATIONS.md
# Uncomment and adjust these if you need custom timeout values:
# DOCLING_TIMEOUT=300
# KG_EXTRACTION_TIMEOUT=3600  
# OPENAI_TIMEOUT=120.0  # For OpenAI LLM requests
# OLLAMA_TIMEOUT=300.0  # For Ollama LLM requests