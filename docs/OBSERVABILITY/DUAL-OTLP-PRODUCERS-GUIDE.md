# DUAL OTLP Producers - Best of Both Worlds! ğŸš€

**Date:** December 30, 2025  
**Status:** RECOMMENDED APPROACH

---

## ğŸ¯ The Best Solution: Run Both!

**Key Insight from Perplexity:** OpenLIT can run **alongside OpenInference** as just another OTLP producer!

Your OTEL Collector doesn't care how many producers send it data - it just routes:
- **Traces** â†’ Jaeger
- **Metrics** â†’ Prometheus

So why choose? **Get the best of both!**

---

## ğŸŒŸ DUAL Mode Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flexible GraphRAG Application    â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ OpenInferenceâ”‚  â”‚   OpenLIT    â”‚â”‚
â”‚  â”‚ (traces)    â”‚  â”‚(traces+metrics)â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ OTEL Collector â”‚
          â”‚  (OTLP 4318)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                     â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Jaeger â”‚         â”‚ Prometheus â”‚
   â”‚(traces) â”‚         â”‚ (metrics)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                              â†“
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Grafana   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… OpenInference: Detailed LlamaIndex traces
- âœ… OpenLIT: Token metrics, costs, VectorDB metrics
- âœ… Custom metrics: Graph extraction, retrieval
- âœ… **No conflicts** - they coexist perfectly!

---

## ğŸš€ Quick Setup (3 minutes!)

### Step 1: Install OpenLIT

```bash
pip install openlit
```

### Step 2: Enable DUAL Mode

**Option A: Environment Variable** (Recommended)
```bash
# In .env
OBSERVABILITY_BACKEND=both
```

**Option B: Docker Compose**
```yaml
services:
  flexible-graphrag:
    environment:
      - OBSERVABILITY_BACKEND=both
```

**Option C: Code** (if you call setup directly)
```python
setup_observability(backend="both")
```

### Step 3: Restart Application

```bash
# Your app will automatically initialize both backends
# Check logs for:
# ğŸš€ Setting up observability with backend: both
# âœ… OpenLIT active - token metrics enabled!
# âœ… OpenInference instrumentation enabled
# ğŸ‰ Observability setup complete - DUAL MODE
```

That's it! ğŸ‰

---

## ğŸ“Š What You Get

### From OpenLIT (Automatic!)

```promql
# Token metrics (GUARANTEED!)
gen_ai_usage_input_tokens_total{gen_ai_request_model="gpt-4"}
gen_ai_usage_output_tokens_total{gen_ai_request_model="gpt-4"}

# Request counts by model
gen_ai_total_requests{gen_ai_request_model="gpt-4"}

# Cost tracking (bonus!)
gen_ai_total_cost{gen_ai_request_model="gpt-4"}

# Latency histograms
gen_ai_request_duration_seconds_bucket{gen_ai_request_model="gpt-4"}

# VectorDB metrics (bonus!)
db_total_requests{db_system="neo4j"}
```

### From OpenInference (Still Active!)

```
# Detailed traces in Jaeger
- All LlamaIndex operations
- Rich span attributes
- Token counts as attributes
```

### From Your Custom Metrics (Still Work!)

```promql
# Graph extraction
rag_graph_entities_extracted_total
rag_graph_relations_extracted_total

# Search/retrieval
rag_retrieval_latency_ms
rag_retrieval_num_documents

# LLM operations
llm_generation_latency_ms
llm_requests_total
```

---

## ğŸ¨ Grafana Dashboard Updates

### Update "Tokens Generated/sec" Panel

**Before (showing 0):**
```promql
sum(rate(llm_tokens_generated_total[5m]))
```

**After (works!):**
```promql
# Prompt + Completion tokens/sec from OpenLIT
sum(rate(gen_ai_usage_input_tokens_total[5m])) + 
sum(rate(gen_ai_usage_output_tokens_total[5m]))
```

### Add New Panels (Bonus!)

**Cost per Hour:**
```promql
sum(rate(gen_ai_total_cost[1h])) * 3600
```

**Requests by Model:**
```promql
sum by (gen_ai_request_model) (rate(gen_ai_total_requests[5m]))
```

**VectorDB Operations:**
```promql
sum by (db_system) (rate(db_total_requests[5m]))
```

### Or Import Pre-Built Dashboard

Visit: https://docs.openlit.io/latest/sdk/destinations/prometheus-jaeger#3-import-the-pre-built-dashboard

---

## ğŸ”§ OTEL Collector (No Changes Needed!)

Your existing config works perfectly! Both OpenInference and OpenLIT send to the same OTLP endpoint.

```yaml
receivers:
  otlp:
    protocols:
      http:
        endpoint: "0.0.0.0:4318"  # Both send here!
      grpc:
        endpoint: "0.0.0.0:4317"

# Spanmetrics still works but now optional
# OpenLIT sends token metrics directly!

service:
  pipelines:
    traces:
      receivers: [otlp]  # Receives from both!
      exporters: [otlp/jaeger, spanmetrics]
    
    metrics:
      receivers: [otlp, spanmetrics]  # Receives from both + spanmetrics!
      exporters: [prometheus]
```

---

## âœ… Verification Steps

### 1. Check Application Logs

```bash
# Should see:
ğŸš€ Setting up observability with backend: both
ğŸ“¡ Initializing OpenLIT as OTLP producer...
âœ… OpenLIT active - token metrics enabled!
ğŸ“¡ Initializing OpenInference as additional OTLP producer...
âœ… OpenInference instrumentation enabled
âœ… Custom RAG metrics initialized
ğŸ‰ Observability setup complete - DUAL MODE
   ğŸ“Š OpenLIT â†’ Token metrics, costs, VectorDB metrics
   ğŸ“Š OpenInference â†’ Detailed traces
   ğŸ“Š Custom metrics â†’ Graph extraction, retrieval, etc.
   ğŸ¯ Best of both worlds!
```

### 2. Check Prometheus Metrics

Visit http://localhost:9090 and search for:

**OpenLIT metrics:**
- `gen_ai_usage_input_tokens_total` âœ…
- `gen_ai_usage_output_tokens_total` âœ…
- `gen_ai_total_requests` âœ…
- `gen_ai_total_cost` âœ…

**Custom metrics (still work):**
- `rag_graph_entities_extracted_total` âœ…
- `rag_graph_relations_extracted_total` âœ…

**Spanmetrics (bonus from OpenInference):**
- `calls_total` âœ…
- `duration_milliseconds_bucket` âœ…

### 3. Check Jaeger Traces

Visit http://localhost:16686

**Should see traces from:**
- OpenInference (llama_index.* spans)
- OpenLIT (gen_ai.* spans)

Both coexist! ğŸ‰

---

## ğŸ¤” FAQ

### Q: Won't this create duplicate data?

**A:** No! They send **complementary** data:
- **OpenLIT:** Focuses on LLM-specific metrics (tokens, costs)
- **OpenInference:** Focuses on detailed traces
- **No conflicts** - different metric names and purposes

### Q: Is this inefficient?

**A:** Negligible overhead:
- Both are lightweight OpenTelemetry producers
- OTLP protocol is efficient
- Collector handles deduplication if needed
- **Benefits far outweigh minimal overhead**

### Q: Can I disable one later?

**A:** Yes! Just change the env var:
```bash
OBSERVABILITY_BACKEND=openinference  # OpenInference only
OBSERVABILITY_BACKEND=openlit        # OpenLIT only
OBSERVABILITY_BACKEND=both           # Both (recommended!)
```

### Q: Do I still need spanmetrics?

**A:** **Optional** now! OpenLIT already provides token metrics directly.

But keep it for now - it provides additional call count and latency metrics that complement the data.

---

## ğŸ¯ Comparison: Single vs DUAL Mode

| Metric/Feature | OpenInference Only | OpenLIT Only | **DUAL Mode** |
|----------------|-------------------|--------------|---------------|
| Token metrics | âŒ Need spanmetrics | âœ… Direct | âœ… **Direct** |
| Detailed traces | âœ… Rich | âœ… Good | âœ… **Rich** |
| Cost tracking | âŒ No | âœ… Yes | âœ… **Yes** |
| VectorDB metrics | âŒ No | âœ… Yes | âœ… **Yes** |
| Custom metrics | âœ… Yes | âœ… Yes | âœ… **Yes** |
| Setup complexity | Medium | Low | **Low** |
| Flexibility | Low | Low | âœ… **High** |

**Winner:** DUAL MODE! ğŸ†

---

## ğŸ“ˆ Real-World Benefits

### Scenario 1: Debugging Performance Issues
- **OpenInference traces:** See exactly where slowdown occurs
- **OpenLIT metrics:** Confirm if it's LLM latency or token volume
- **Custom metrics:** Check if graph extraction is the bottleneck

### Scenario 2: Cost Optimization
- **OpenLIT costs:** Track spend by model in Grafana
- **OpenLIT tokens:** Identify expensive queries
- **OpenInference traces:** Find unnecessary LLM calls in code

### Scenario 3: Production Monitoring
- **OpenLIT dashboard:** High-level LLM health (tokens/sec, cost/hour, errors)
- **Custom metrics:** RAG-specific KPIs (entities extracted, retrieval latency)
- **OpenInference traces:** Deep dive when issues occur

---

## ğŸš€ Migration Path

### If You're Currently Using OpenInference

```bash
# Just add OpenLIT!
pip install openlit
export OBSERVABILITY_BACKEND=both
# Restart - instant token metrics!
```

### If You're Currently Using OpenLIT

```bash
# OpenInference should already be installed
# Just enable both mode
export OBSERVABILITY_BACKEND=both
# Restart - get richer traces!
```

### If You're Starting Fresh

```bash
pip install openlit openinference-instrumentation-llama-index
export OBSERVABILITY_BACKEND=both
# Perfect from day 1!
```

---

## ğŸ“š References

- [OpenLIT with OTEL Collector](https://docs.openlit.io/latest/sdk/destinations/otelcol) - "OpenLIT just becomes another OTLP producer"
- [OpenLIT + Prometheus + Jaeger](https://docs.openlit.io/latest/sdk/destinations/prometheus-jaeger)
- [Grafana LLM Observability Guide](https://grafana.com/blog/a-complete-guide-to-llm-observability-with-opentelemetry-and-grafana-cloud/)
- [OpenInference Semantic Conventions](https://arize-ai.github.io/openinference/spec/semantic_conventions.html)

---

## âœ… Summary

**DUAL mode is the best approach because:**

1. âœ… **Guaranteed token metrics** (from OpenLIT)
2. âœ… **Rich traces** (from OpenInference)
3. âœ… **Cost tracking** (from OpenLIT)
4. âœ… **Custom metrics** (still work)
5. âœ… **No conflicts** (complementary data)
6. âœ… **Easy setup** (one env var)
7. âœ… **Future-proof** (can disable either later)

**Just do this:**
```bash
pip install openlit
export OBSERVABILITY_BACKEND=both
# Restart app
# Check Prometheus for gen_ai_usage_*_tokens_total
# Profit! ğŸ‰
```

---

**Status:** DUAL mode implemented and recommended! ğŸš€  
**Your token metrics problem is SOLVED** with the best possible solution!


